{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be5dd1c1-e45b-4126-a4f4-44a73b14086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import gensim.models\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import MeCab\n",
    "import ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3124077c-fc7d-475d-87dd-0492323c7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "b_tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "b_model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "m_tagger = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
    "\n",
    "cm = gensim.models.KeyedVectors.load_word2vec_format(\"assets/cc.ja.300.vec.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dafa386e-f04e-4948-bf72-a3d51b7efba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tokenizer = MeCab.Tagger('-O wakati ' + ipadic.MECAB_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f22a02e-cea8-4dca-8f7f-6d3d102ce65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_text(text, mecab_tokenizer, bert_tokenizer):\n",
    "\n",
    "    \"\"\"文を受け取りMeCabとBERTトークナイザの結果からマスク位置を決定，マスクされた文のリストを出力\"\"\"\n",
    "    \n",
    "    mask_labels = [] # ['r:そのまま', 'm:マスクする', 'h:まとめてマスクする', 's:まとめられる語']\n",
    "    ogl_mecab_tokens = []\n",
    "    masked_texts = []\n",
    "    target_tokens = []\n",
    "    mecab_tokens_list = []\n",
    "    zure = 0\n",
    "    \n",
    "    node = mecab_tokenizer.parseToNode(text)\n",
    "    tokenized_text = bert_tokenizer.tokenize(text)\n",
    "    \n",
    "    node = node.next #文頭，文末形態素を無視\n",
    "    while node.next:\n",
    "        \n",
    "        #print(node.feature)\n",
    "        \n",
    "        if node.surface not in  tokenized_text: #サブワード化された語を変換対象から外すため\n",
    "            mask_labels.append('r')\n",
    "        elif node.feature.split(',')[0] in ['助詞', '助動詞', '連体詞', '形容詞', '記号']:\n",
    "            mask_labels.append('r')\n",
    "        elif node.feature.split(',')[1] == 'サ変接続':\n",
    "            mask_labels.append('h')\n",
    "        elif node.feature.split(',')[4] == 'サ変・スル' and mask_labels[-1] == 'h':\n",
    "            mask_labels.append('s')\n",
    "        else:\n",
    "            mask_labels.append('m')\n",
    "        ogl_mecab_tokens.append(node.surface)\n",
    "        node = node.next\n",
    "        \n",
    "    #print(mask_labels)\n",
    "    \n",
    "    memory_masked = ogl_mecab_tokens[:]\n",
    "    \n",
    "    for target_idx, mask_label in enumerate(mask_labels):\n",
    "        mecab_tokens = ogl_mecab_tokens[:]\n",
    "        \n",
    "        target_token = mecab_tokens[target_idx]\n",
    "        \n",
    "        if mask_label == 'm':\n",
    "            mecab_tokens[target_idx] = '[MASK]'\n",
    "            memory_masked[target_idx-zure] = '[MASK]'\n",
    "        elif mask_label == 'h':\n",
    "            mecab_tokens[target_idx] = '[MASK]'\n",
    "            memory_masked[target_idx-zure] = '[MASK]'\n",
    "            for m_label in mask_labels[target_idx+1:]:\n",
    "                if m_label == \"s\":\n",
    "                    mecab_tokens.pop(target_idx+1)\n",
    "                    memory_masked.pop(target_idx+1)\n",
    "                    zure = zure + 1\n",
    "                else:\n",
    "                    break\n",
    "        elif mask_label in ['r','s']:\n",
    "            continue\n",
    "        \n",
    "        mecab_tokens_list.append(mecab_tokens[:])\n",
    "        \n",
    "        masked_text = \"\".join(mecab_tokens)\n",
    "        \n",
    "        masked_texts.append(masked_text)\n",
    "        target_tokens.append(target_token)\n",
    "        \n",
    "        mecab_tokens[target_idx] = target_token\n",
    "        \n",
    "    \n",
    "    return masked_texts, mecab_tokens_list, target_tokens, memory_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3ec5d0-2f59-4cf6-9c58-b09f69a0dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(text, masked_text, bert_tokenizer, bert_model):\n",
    "    \n",
    "    \"\"\"マスクされた文と原文を受け取って候補を10語出力\"\"\"\n",
    "    \n",
    "    encoded_dict = bert_tokenizer(text, masked_text)\n",
    "\n",
    "    masked_idx = encoded_dict[\"input_ids\"].index(4)\n",
    "    tokens_tensor = torch.tensor([encoded_dict[\"input_ids\"]])\n",
    "    segments_tensors = torch.tensor([encoded_dict[\"token_type_ids\"]])\n",
    "\n",
    "    bert_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        segments_tensors = segments_tensors.to('cuda')\n",
    "        bert_model.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    topk_score, topk_index = torch.topk(predictions[0, masked_idx], 10)\n",
    "    topk_tokens = bert_tokenizer.convert_ids_to_tokens(topk_index.tolist())\n",
    "    \n",
    "    bert_rank = np.array([i for i in range(len(topk_tokens))])\n",
    "    \n",
    "    topk_data = {\"tokens\": topk_tokens, \"score\": topk_score, \"index\": topk_index, \"rank\": bert_rank}\n",
    "    \n",
    "    return topk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc48520-f5fa-45ad-96a3-fdfaf7f97e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_ranking(candidates):\n",
    "    \n",
    "    \"\"\"頻度表を検索，高い順からランキングします\"\"\"\n",
    "    \n",
    "    if os.path.isfile(\"assets/bccwj_frequency.pickle\") == 'True':\n",
    "        bccwj_frequency = {}\n",
    "        with open(\"assets/BCCWJ_goihyo_utf8.txt\") as BCCWJ:\n",
    "            for BCCWJ_line in BCCWJ:\n",
    "                BCCWJ_line_list = BCCWJ_line.split('\\t')\n",
    "                if BCCWJ_line_list[0] != 'ID_BCCWJ':\n",
    "                    bccwj_frequency[BCCWJ_line_list[3]] = sum(int(i) for i in BCCWJ_line_list[9:15])\n",
    "        \n",
    "        with open(\"assets/bccwj_frequency.pickle\",'wb') as f:\n",
    "            pickle.dump(bccwj_frequency, f)\n",
    "        \n",
    "    with open(\"assets/bccwj_frequency.pickle\", 'rb') as f:\n",
    "        bccwj_frequency = pickle.load(f)\n",
    "    \n",
    "    candidates_frequency = [bccwj_frequency.get(cand, 0) for cand in candidates]\n",
    "    frequency_rank = np.array([np.argsort([-int(freq) for freq in candidates_frequency]).tolist().index(idx) for idx in range(10)]) #ランク付けの呪文\n",
    "    \n",
    "    frequency_data = {'frequency': candidates_frequency, 'rank': frequency_rank}\n",
    "    \n",
    "    return frequency_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f943cbd0-5722-4ec3-bbc2-706da7a3f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_ranking_twc(candidates, tkz_mkd_text, mecab_tagger):\n",
    "    cand_genkeis = []\n",
    "    \n",
    "    if os.path.isfile(\"./assets/nlt_frequency.pickle\"):\n",
    "        with open(\"assets/nlt_frequency.pickle\", 'rb') as f:\n",
    "            nlt_frequency = pickle.load(f)\n",
    "    else:\n",
    "        nlt_frequency = {}\n",
    "        with open(\"assets/NLT_freq_list_split.txt\") as NLT:\n",
    "            for NLT_line in NLT.read().splitlines():\n",
    "                NLT_line_list = NLT_line.split(',')\n",
    "                num = NLT_line_list[1]\n",
    "                nlt_frequency[NLT_line_list[0]] = num\n",
    "        \n",
    "        with open(\"assets/nlt_frequency.pickle\", 'wb') as f:\n",
    "            pickle.dump(nlt_frequency, f)\n",
    "    \n",
    "    masked_idx = tkz_mkd_text.index(\"[MASK]\")\n",
    "    \n",
    "    for cand in candidates:\n",
    "        wak_idx = 0\n",
    "        \n",
    "        tkz_mkd_text[masked_idx] = cand\n",
    "        \n",
    "        node = mecab_tagger.parseToNode(\"\".join(tkz_mkd_text))\n",
    "        node = node.next\n",
    "        while node.next:\n",
    "            if wak_idx == masked_idx:\n",
    "                cand_genkeis.append(node.feature.split(',')[6])\n",
    "            wak_idx = wak_idx + 1\n",
    "            node = node.next    \n",
    "    \n",
    "    candidates_frequency = [nlt_frequency.get(cand, 0) for cand in cand_genkeis]\n",
    "    frequency_rank = np.array([np.argsort([-int(freq) for freq in candidates_frequency]).tolist().index(idx) for idx in range(10)])\n",
    "    \n",
    "    frequency_data = {'frequency': candidates_frequency, 'rank': frequency_rank}\n",
    "    \n",
    "    return frequency_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e537eae7-71b9-4a2c-9376-52f4416a18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_similarity_ranking(target_token, candidates, fasttext_vec):\n",
    "    \n",
    "    \"\"\"Cos類似度を求めてランク付けする．gensimで読み込んでるんだったら他も使えるかも\"\"\"\n",
    "    \n",
    "    cosine_sim = [cm.similarity(target_token, cand) if target_token in cm and cand in cm else 0 for cand in candidates]\n",
    "    \n",
    "    sim_rank = np.array([np.argsort([-sim for sim in cosine_sim]).tolist().index(idx) for idx in range(10)]) #ランク付けの呪文\n",
    "    \n",
    "    sim_data = {\"cos_sim\": cosine_sim, \"rank\": sim_rank}\n",
    "    \n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9eeebe5f-37b2-404b-8d68-78ea5cb19e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heiika(text):\n",
    "    simple_words = []\n",
    "    \n",
    "    masked_list, mkb_tok_list, target_tokens, memory_masked = masking_text(text, m_tagger, b_tokenizer)\n",
    "    \n",
    "    for text_idx, masked_text in enumerate(masked_list):\n",
    "        topk_dict = get_candidates(text, masked_text, b_tokenizer, b_model)\n",
    "\n",
    "        #freq_data = frequency_ranking(topk_dict[\"tokens\"])\n",
    "        freq_data = frequency_ranking_twc(topk_dict[\"tokens\"], mkb_tok_list[text_idx], m_tagger)\n",
    "\n",
    "        sim_dict = fasttext_similarity_ranking(target_tokens[text_idx], topk_dict[\"tokens\"], cm)\n",
    "\n",
    "        avg_idxs = np.argsort(topk_dict[\"rank\"] + freq_data[\"rank\"] + sim_dict[\"rank\"]) #合算したランクが同じになったらBERTスコアが優先されます\n",
    "        \n",
    "        sorted_candidates = [topk_dict[\"tokens\"][a_idx] for a_idx in avg_idxs]\n",
    "\n",
    "        simple_words.append(sorted_candidates[0])\n",
    "        \n",
    "        print(topk_dict[\"tokens\"])\n",
    "        print(topk_dict[\"score\"])\n",
    "        print(freq_data[\"frequency\"])\n",
    "        print(sim_dict[\"cos_sim\"])\n",
    "    \n",
    "    kantannabun = []\n",
    "    sw_idx = 0\n",
    "    for tok in memory_masked:\n",
    "        if tok == \"[MASK]\":\n",
    "            kantannabun.append(simple_words[sw_idx])\n",
    "            sw_idx = sw_idx + 1\n",
    "        else:\n",
    "            kantannabun.append(tok)\n",
    "\n",
    "    print(\"\".join(kantannabun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d7b00dde-126f-4955-b38a-e3fd63f19bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['若者', '大人', '子供', '子ども', 'アスリート', '青年', '企業', '女性', 'スポーツ', '老人']\n",
      "tensor([12.6665, 10.3220,  9.9363,  9.8606,  9.3604,  9.3049,  9.1614,  8.9949,\n",
      "         8.9760,  8.8435])\n",
      "['44767', '85155', '676310', 0, '1', '34469', '463441', '341491', '68111', '35918']\n",
      "[1.0, 0.49588507, 0.5431058, 0.5366304, 0.37994212, 0.54304886, 0.4340958, 0.5138575, 0.3189348, 0.5010498]\n",
      "['未来', '過去', '次世代', '現代', '時代', '明日', '現在', '自然', '日常', '人生']\n",
      "tensor([13.4072, 11.7603, 10.8013, 10.4733, 10.3210,  9.6666,  9.6420,  9.5709,\n",
      "         9.1504,  9.0598])\n",
      "['69384', '132759', '13938', '90122', '360825', '51071', '452559', '301544', '97181', '155473']\n",
      "[1.0, 0.45295668, 0.5192402, 0.41967815, 0.33084735, 0.3713258, 0.21572639, 0.32597736, 0.2618374, 0.39183265]\n",
      "['担う', '支える', '担い', '伝える', '引き継ぐ', '育てる', '語る', '考える', '演じる', '生きる']\n",
      "tensor([17.1996, 12.2899, 11.6165, 11.5818, 11.2846, 11.1931, 11.1063, 10.7688,\n",
      "        10.6419, 10.6276])\n",
      "['45554', '70429', '45554', '177123', '17360', '79384', '109779', '1195718', '24091', '222178']\n",
      "[1.0, 0.6483965, 0.6130984, 0.45741898, 0.5825057, 0.4828217, 0.4386239, 0.4747821, 0.4647299, 0.29216456]\n",
      "子供が未来を担う\n"
     ]
    }
   ],
   "source": [
    "heiika(\"若者が未来を担う\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bda3f1-5cef-4f81-a964-c452c1f6c376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sds",
   "language": "python",
   "name": "sds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
