{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5dd1c1-e45b-4126-a4f4-44a73b14086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawahara/.conda/envs/sds/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gensim.models\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import MeCab\n",
    "import ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3124077c-fc7d-475d-87dd-0492323c7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "b_model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "m_tokenizer = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
    "cm = gensim.models.KeyedVectors.load_word2vec_format(\"assets/cc.ja.300.vec.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4f22a02e-cea8-4dca-8f7f-6d3d102ce65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_text(text, mecab_tokenizer, bert_tokenizer):\n",
    "\n",
    "    \"\"\"文を受け取りMeCabとBERTトークナイザの結果からマスク位置を決定，マスクされた文のリストを出力\"\"\"\n",
    "    \n",
    "    mask_labels = [] # ['r:そのまま', 'm:マスクする', 'h:まとめてマスクする', 's:まとめられる語']\n",
    "    ogl_mecab_tokens = []\n",
    "    masked_texts = []\n",
    "    target_tokens = []\n",
    "    zure = 0\n",
    "    \n",
    "    node = mecab_tokenizer.parseToNode(text)\n",
    "    tokenized_text = bert_tokenizer.tokenize(text)\n",
    "    \n",
    "    node = node.next #文頭，文末形態素を無視\n",
    "    while node.next:\n",
    "        \n",
    "        #print(node.feature)\n",
    "        \n",
    "        if node.surface not in  tokenized_text: #サブワード化された語を変換対象から外すため\n",
    "            mask_labels.append('r')\n",
    "        elif node.feature.split(',')[0] in ['助詞', '助動詞', '連体詞', '形容詞', '記号']:\n",
    "            mask_labels.append('r')\n",
    "        elif node.feature.split(',')[1] == 'サ変接続':\n",
    "            mask_labels.append('h')\n",
    "        elif node.feature.split(',')[4] == 'サ変・スル' and mask_labels[-1] == 'h':\n",
    "            mask_labels.append('s')\n",
    "        else:\n",
    "            mask_labels.append('m')\n",
    "        ogl_mecab_tokens.append(node.surface)\n",
    "        node = node.next\n",
    "        \n",
    "    #print(mask_labels)\n",
    "    \n",
    "    memory_masked = ogl_mecab_tokens[:]\n",
    "    \n",
    "    for target_idx, mask_label in enumerate(mask_labels):\n",
    "        mecab_tokens = ogl_mecab_tokens[:]\n",
    "        \n",
    "        target_token = mecab_tokens[target_idx]\n",
    "        \n",
    "        if mask_label == 'm':\n",
    "            mecab_tokens[target_idx] = '[MASK]'\n",
    "            memory_masked[target_idx-zure] = '[MASK]'\n",
    "        elif mask_label == 'h':\n",
    "            mecab_tokens[target_idx] = '[MASK]'\n",
    "            memory_masked[target_idx-zure] = '[MASK]'\n",
    "            for m_label in mask_labels[target_idx+1:]:\n",
    "                if m_label == \"s\":\n",
    "                    mecab_tokens.pop(target_idx+1)\n",
    "                    memory_masked.pop(target_idx+1)\n",
    "                    zure = zure + 1\n",
    "                else:\n",
    "                    break\n",
    "        elif mask_label in ['r','s']:\n",
    "            continue\n",
    "        \n",
    "        masked_text = \"\".join(mecab_tokens)\n",
    "        \n",
    "        masked_texts.append(masked_text)\n",
    "        target_tokens.append(target_token)\n",
    "        \n",
    "        mecab_tokens[target_idx] = target_token\n",
    "        \n",
    "    \n",
    "    return masked_texts, target_tokens, memory_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0a3ec5d0-2f59-4cf6-9c58-b09f69a0dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(text, masked_text, bert_tokenizer, bert_model):\n",
    "    \n",
    "    \"\"\"マスクされた文と原文を受け取って候補を10語出力\"\"\"\n",
    "    \n",
    "    encoded_dict = bert_tokenizer(text, masked_text)\n",
    "\n",
    "    masked_idx = encoded_dict[\"input_ids\"].index(4)\n",
    "    tokens_tensor = torch.tensor([encoded_dict[\"input_ids\"]])\n",
    "    segments_tensors = torch.tensor([encoded_dict[\"token_type_ids\"]])\n",
    "\n",
    "    bert_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        segments_tensors = segments_tensors.to('cuda')\n",
    "        bert_model.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    topk_score, topk_index = torch.topk(predictions[0, masked_idx], 10)\n",
    "    topk_tokens = bert_tokenizer.convert_ids_to_tokens(topk_index.tolist())\n",
    "    \n",
    "    bert_rank = np.array([i for i in range(len(topk_tokens))])\n",
    "    \n",
    "    topk_data = {\"tokens\": topk_tokens, \"score\": topk_score, \"index\": topk_index, \"rank\": bert_rank}\n",
    "    \n",
    "    return topk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7fc48520-f5fa-45ad-96a3-fdfaf7f97e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_ranking(candidates):\n",
    "    \n",
    "    \"\"\"頻度表を検索，高い順からランキングします\"\"\"\n",
    "    \n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    if os.path.isfile('assets/bccwj_frequency.pickle') == 'True':\n",
    "        bccwj_frequency = {}\n",
    "        with open('assets/BCCWJ_goihyo_utf8.txt') as BCCWJ:\n",
    "            for BCCWJ_line in BCCWJ:\n",
    "                BCCWJ_line_list = BCCWJ_line.split('\\t')\n",
    "                if BCCWJ_line_list[0] != 'ID_BCCWJ':\n",
    "                    bccwj_frequency[BCCWJ_line_list[3]] = sum(int(i) for i in BCCWJ_line_list[9:15])\n",
    "        \n",
    "        with open(\"assets/bccwj_frequency.pickle\",\"wb\") as f:\n",
    "            pickle.dump(bccwj_frequency, f)\n",
    "        \n",
    "    with open('assets/bccwj_frequency.pickle', 'rb') as f:\n",
    "        bccwj_frequency = pickle.load(f)\n",
    "    \n",
    "    candidates_frequency = [bccwj_frequency.get(cand, 0) for cand in candidates]\n",
    "    frequency_rank = np.array([np.argsort([-int(freq) for freq in candidates_frequency]).tolist().index(idx) for idx in range(10)]) #ランク付けの呪文\n",
    "    \n",
    "    frequency_data = {\"frequency\":candidates_frequency, \"rank\":frequency_rank,}\n",
    "    \n",
    "    return frequency_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e537eae7-71b9-4a2c-9376-52f4416a18a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_similarity_ranking(target_token, candidates, fasttext_vec):\n",
    "    \n",
    "    \"\"\"Cos類似度を求めてランク付けする．gensimで読み込んでるんだったら他も使えるかも\"\"\"\n",
    "    \n",
    "    cosine_sim = [cm.similarity(target_token, cand) if target_token in cm and cand in cm else 0 for cand in candidates]\n",
    "    \n",
    "    sim_rank = np.array([np.argsort([-sim for sim in cosine_sim]).tolist().index(idx) for idx in range(10)]) #ランク付けの呪文\n",
    "    \n",
    "    sim_data = {\"cos_sim\": cosine_sim, \"rank\": sim_rank}\n",
    "    \n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9eeebe5f-37b2-404b-8d68-78ea5cb19e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heiika(text):\n",
    "    simple_words = []\n",
    "    \n",
    "    masked_list, target_tokens, memory_masked = masking_text(text, m_tokenizer, b_tokenizer)\n",
    "    \n",
    "    for text_idx, masked_text in enumerate(masked_list):\n",
    "        topk_dict = get_candidates(text, masked_text, b_tokenizer, b_model)\n",
    "\n",
    "        freq_data = frequency_ranking(topk_dict[\"tokens\"])\n",
    "\n",
    "        sim_dict = fasttext_similarity_ranking(target_tokens[text_idx], topk_dict[\"tokens\"], cm)\n",
    "\n",
    "        avg_idxs = np.argsort(topk_dict[\"rank\"] + freq_data[\"rank\"] + sim_dict[\"rank\"]) #合算したランクが同じになったらBERTスコアが優先されます\n",
    "        \n",
    "        sorted_candidates = [topk_dict[\"tokens\"][a_idx] for a_idx in avg_idxs]\n",
    "\n",
    "        simple_words.append(sorted_candidates[0])\n",
    "\n",
    "    kantannabun = []\n",
    "    sw_idx = 0\n",
    "    for tok in memory_masked:\n",
    "        if tok == \"[MASK]\":\n",
    "            kantannabun.append(simple_words[sw_idx])\n",
    "            sw_idx = sw_idx + 1\n",
    "        else:\n",
    "            kantannabun.append(tok)\n",
    "\n",
    "    print(\"\".join(kantannabun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d7b00dde-126f-4955-b38a-e3fd63f19bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "七夕にきさらぎ駅へ訪れた\n"
     ]
    }
   ],
   "source": [
    "heiika(\"七夕にきさらぎ駅へ訪れた\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bda3f1-5cef-4f81-a964-c452c1f6c376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sds",
   "language": "python",
   "name": "sds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
